# Geti Inference

Inference app to automate inference processes



* Internal release opens
** DONE remove public models for now
** DONE Implement switch between devices (cpu, igpu, dgpu, npu)
** DONE Implement new genAI interface
** DONE Work on project import
*** NO Check that huggingface/Openvino models can be imported
*** DONE Allow import of model.xml directly
assume that the binary is next to it.
Labels colors should be added
*** DONE Model.xml zip file import
**** NO Check if task chain is available using a separate file (pipeline.json or something)
** DONE Clean up the model info list (accuracy etc)
** Cleanup:
*** DONE Fix up llm provider dispose asking during text generation
The idea is that it should make sure that the isolate worker can return true to stop it and only then the provider should be disposed
*** DONE Move the device selection to next to the clear, or the upload image.
*** DONE Add check for zip size and show message to extract first
*** DONE Load LLM model before showing text area.
*** DONE folder selector instead
*** DONE remove auto device
*** DONE Accept geti deployment as directory instead of zip
*** DONE Check if the gpu is being used for serialization.
*** DONE Improve the too big message to include project.json
Just removed it since im no longer importing zips
*** DONE token time instead of inference speed.
*** DONE Perhaps pull apart the history in LLM from the pipe. Clear could just reset the history instead
Easy implement of clearHistory method in bindings
*** DONE change the icon so its pretty in the taskbar
*** DONE Enter button icon for llm needs to be disabled when unavailable and prettier
*** DONE Change  type a message to assistant -> something like "type a message to ${name}"
*** DONE Choose between ux for model downloading. In inference page or projects page.
*** DONE pull apart batch inference provider and image inference provider stuff
This is now coupled, however it might be nice to just load the model on the go so that image inference is a lot faster.
*** DONE nice public model loading
*** NO Add size information for public models
This is not available information for now, but it would help?
*** DONE use huggingface api instead of our fixed list.
*** OKAY Improve ux for downloading
**** show progress in megabytes
**** add stop button

** UI team meeting:
*** DONE show nr downloads in public models
*** DONE add likes
*** DONE grid view or list view

*** DONE use a modal for adding a new model. Then select a public
**** DONE - if not, perhaps opacity.
**** DONE allow selecting of public
***** DONE Implement downloading after selection.

**** DONE allow selecting of zip
**** NO allow selecting of directory
*** DONE Show active filters


** DONE Style grid a bit better.
*** DONE use logo of specific model
Hardcode them. use an AI generated image as fallback
** DONE Sort by name
** DONE only support huggingface & geti deployments for now
Drop the local (via directory)
** DONE Make import model a dropdown
** DONE think about selecting multiple? (SELECT ONE at the time)
One at the time makes more sense. Parallel will be slower for the user.
Go to the model right away and show download progress
** DONE Make download show without decimal pointer
** DONE Fix downloader when cancelling
Not sure how to fix this yet
** DONE Check all geti models
** DONE Filter optimization in public models instead


* Concepts
** Project
A project can be a geti project deployment or a new project made from a public model
It will create a folder on your application support directory with a uuid as name
Every project will usually contain one model or a geti deployment.

When you upload a model or select one from a public
** Deployment
Geti produces deployments.

** Model
A model contains
** Serialization
** GraphRunner
** Graphs
